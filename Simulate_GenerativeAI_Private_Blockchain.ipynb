# Preserving Generative AI Text on Blockchain through Simulation

## Overview

Llama 2 is a state-of-the-art collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters. Optimized for dialogue and conversational use cases, it surpasses most open-source chat models and competes with leading closed-source models in human evaluations, particularly in areas of helpfulness and safety.

This project demonstrates how to use Llama 2, specifically the **Llama 2 13B-chat** model, integrated with **llama.cpp** for efficient execution on consumer hardware, such as MacBook and x86 architectures, through integer quantization and various BLAS libraries.

## Features
- **Llama 2**: A high-performance model for generating dialogue responses.
- **llama.cpp**: Optimized C/C++ implementation to run models with 4-bit integer quantization on Apple Silicon and x86 machines.
- **GGML Library**: A machine learning library that enables the distribution and execution of large language models (LLMs) on consumer hardware by reducing precision for better resource efficiency.
  
## Setup

### Step 1: Install Required Packages

First, install the necessary Python libraries to interact with the model:

```bash
!pip install pymongo llama-cpp-python==0.1.78 huggingface_hub numpy
```

### Step 2: Import the Required Libraries

```python
from huggingface_hub import hf_hub_download
from llama_cpp import Llama
```

### Step 3: Download the Model

Use Hugging Face Hub to download the model:

```python
model_name_or_path = "TheBloke/Llama-2-13B-chat-GGML"
model_basename = "llama-2-13b-chat.ggmlv3.q5_1.bin"

m_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)
```

### Step 4: Load the Model

Load the Llama model for inference:

```python
lcpp_llm = Llama(
    model_path=m_path,
    n_threads=2, 
    n_batch=512, 
    n_gpu_layers=32 
)
```

### Step 5: Create a Prompt Template

```python
prompt = "Where is the largest country in the world?"
prompt_template = f'''
USER: {prompt}

ASSISTANT:
'''
```

### Step 6: Generate a Response

Run the model to generate text:

```python
response = lcpp_llm(
    prompt=prompt_template, 
    max_tokens=256, 
    temperature=0.5, 
    top_p=0.95, 
    repeat_penalty=1.2, 
    top_k=150, 
    echo=True
)

print(response["choices"][0]["text"])
```

## Blockchain Simulator (Optional)

As part of the project, a blockchain simulator is implemented using **MongoDB**. Follow these additional steps if interested:

### Step 7.1: Set Up MongoDB Connection

```python
from pymongo import MongoClient
import certifi

uri = "mongodb+srv://your-mongo-connection-string"
myclient = MongoClient(uri, tls=True, tlsCAFile=certifi.where())
db = myclient["Generative-AI"]
blocks_collection = db["Blockchain"]
```

### Step 7.2: Create a Genesis Block

```python
def create_genesis_block():
    record = {
        'message': 'Genesis Block',
        'index': 0,
        'timestamp': time.time(),
        'data': 'Genesis Block',
        'previous_hash': '0',
        'hash': calculate_hash(0, '0', time.time(), 'Genesis Block')
    }
    blocks_collection.insert_one(record)
    return record

create_genesis_block()
```

##Contact
Hossein Hosseinalibeiki
Linkedin : https://www.linkedin.com/in/hossein-hosseinalibeiki-1208583b/ 
